\section{Monitoring and Metrics}
\label{monitoring}

To provide an extensive overview on how resources are used in order to guide infrastructure development and to allow users to make an informed decision about which infrastructure to use, key parameters should be published for each AF.

Metrics for AFs can be categorized into four areas

\begin{enumerate}
    \item User experience metrics
    \item User trend metrics
    \item Performance metrics
    \item Facility metrics
\end{enumerate}

User experience metrics relate to the analysts’ impression of the facility. Support for users is a key component of the analyst experience. Such support metrics include time to acknowledge tickets and, as a separate metric, time to resolve tickets. Unlike time to acknowledge tickets, time to resolve tickets is very case specific, for instance, if a ticket requests or requires the implementation of new functionality it will often take considerable time (and not all requests are even reasonable!). Over an extended time period, and with some care, this can be compared between sites. Once new functionality or features are added to a facility the rate of uptake by users is a useful metric to gauge user engagement and is closely related to the findability and usability of documentation and tutorials, which are key for user on-boarding and retention (see also section~\ref{documentation}). User satisfaction surveys can gather feedback directly from users (albeit response rates can be low). 

User trend metrics can be used to inform future infrastructure development and act as a “live survey” of user analysis requirements. This could include metrics on the data source being used (local or cached from a remote site or "DataLake"), input data format, resources employed per job and interactive session length which would impact parameters of the caching system. In an idealized scenario this would mean a common API across software tools used at the analysis infrastructures, but this cannot realistically be imposed.  Tools like prmon~\cite{prmon}, or in general any code profiler that can be run within the user job, are well tested and can easily collect resource usage information while the jobs run and are not tied to any specific software. Users cannot be expected to run this monitoring on their own, so it requires automation from the VOs and/or the sites. When facilities run these tools, wrapping the jobs automatically, they should also set up a visualization infrastructure for the users to track their jobs. This happens on the grid where the WFMS of the experiments have robust monitoring of every job and metrics are then aggregated to give a wide view of the resource usage by site or globally. Another example of this would be the Grafana dashboard at CERN, which displays some information on the user batch jobs. However, it does not cover other resources (cloud or interactive) or many desirable metrics. Of course tracking basic machine level or batch system resources can be done almost automatically. However for tracking each application, what it does and what data it uses there is no other option but to push the users to use an instrumented framework which collects all this extra information.

The concept of dark analysis was raised at the Analysis Ecosystem Workshop II and refers to the analysis that users run privately on, for instance, university or laboratory resources. We currently have no handle on the type and scale of this analysis and therefore a user survey was proposed to analyze these user trends. This kind of large scale survey must be treated carefully. The survey could be conducted through the HSF Data Analysis working group and would give further insight into the analysis needs of users across experiments. If we had instrumented AF the problem of dark analysis would be reduced.

Whilst all current facilities have different focuses, and this is important, there are universal aspects of an analysis pipeline that can serve as benchmarks. These can be used to assess analysis facility performance - a performance metric.  The IRIS-HEP Analysis Grand Challenge (AGC)~\cite{agc} has established complete analysis workflows using CERN Open Data that can be used to test the functionality, integration and performance of an AF. These workflows can be factorized to benchmark performance in specific areas, for instance, data delivery. They also serve as useful tutorials for analysts, showcasing complete, realistic end user analyses that exploit the tools discussed in this document as well as the scientific python ecosystem. 

Publication of facility metrics is considered a necessary aspect. Facility metrics quantify the technical performance of the facility rather than the analysis performance. Resource idle time, job failure rate and downtime should all be monitored by the facility host.

Increasingly, sustainability metrics will be assessed, scrutinized and perform an important role in evaluating facilities’ viability. For instance, one could consider the carbon footprint per user/job. As reported at the 26th International Conference on Computing in High Energy \& Nuclear Physics (CHEP 2023) the environmental footprint of HEP research overall can be controlled via modernization of the facilities we use to run jobs, improvements in the software and computing models of the experiments and improvement in hardware technologies. AFs have the opportunity to influence two of these factors. This will be carefully balanced against operational cost per user/job where the weighting of each of these (potentially conflicting) optimisations will be decided by funding agencies. Facilities must provide information for both and meaningful metrics will evolve over time for all sites, not just AFs.

Infrastructure status reports would be important to track the AF efficiency, but it is a whole project in itself and would take a fair amount of operational overhead to maintain, correct, ensure the system reports fairly, etc. Facilities should certainly track metrics internally as a first step. All analysis infrastructures should define key parameters to measure success over time and a level of coordination on these parameters across infrastructures should be implemented to make fair comparisons. As a minimal set we propose the following as an evolution over time: total number of active users, total compute time spent both on local and on batch resources, storage space available and used, I/O rates, Time-To-Completion and data source (local, cached, remote).
