\section{User perspective}
\label{userperspective}

Analysis Facilities aim to facilitate the preparation and execution of HEP data analyses. During the course of the HSF Analysis Facility Forum a number of requirements have been identified in order to enable users to do their research effectively. 

\subsection*{Ability to perform fast research iterations on large datasets interactively}

Research success relies crucially on the ability to test ideas during the R\&D phase of an analysis, e.g. assessing the impact of a change in event selection or determining the impact of a variation in systematics. Here a fast “time-to-insight”, i.e. minimal wall-clock time, is a priority. In this phase, the user expects to rapidly gain access to and interactively work with the resources. This requirement so far has been satisfied by classic facilities that provide a number of interactive, single “login” nodes accessible via remote terminal/ssh sessions or, more recently, by providing small single node Jupyter~\cite{jupyter} notebooks. 


However dataset sizes are increasing and an interactive experience constrained to a single node with a handful of resources may no longer be sufficient. The requirement becomes to enable a distributed and interactive analysis style, where the work is coordinated by the user in an interactive session, but the user can dynamically connect to a scale-out system that provides additional resources. The expectation is that the processing on additional resources starts in parallel almost immediately, with only a short lag-time due to resource provisioning, and the resulting data is aggregated and sent back to the interactive node for the user to examine in real-time. 

What constitutes “minimal wall-clock time”, “large amount of resources” and “large datasets” is analysis dependent and experiments should define acceptable/expected ranges for their analyses. The implications on how interactivity could be achieved are still under discussion. 

\subsection*{Ability to convert interactive to batch-schedulable workloads}

When an analysis matures from an R\&D phase to performing the final measurement, the usage pattern often changes from a rapid iteration and frequent changes of the analysis to a more controlled pattern, in which results from various configurations must be produced. Here users may not urgently require full interactivity and, in fact, may prefer to prepare a number of analysis configurations ahead of time and asynchronously schedule them in bulk. In this mode, the total wall-clock time required for a processing is not an urgent concern to the user and facility-side optimizations towards resource use efficiency, load balancing, may be valid trade-offs. This mode is typically handled as a non-interactive batch workload, as supported by a workload scheduling system.

As more interactive distributed workloads become possible for the R\&D phase, it is important that it is easy to convert a workload from an interactive setup into a “headless” mode that is amenable to batch systems. This poses requirements on both the analysis software as well as the analysis facility. This of course is mostly important in case of Jupyter notebooks where the submission procedures to a batch system need to be developed by the AFs. A Jupyter notebook that, in interactive mode, dynamically scales to multiple nodes to execute an analysis and is driven by a user working within a notebook, should be re-configurable into a non-interactive workload that requests a fixed number of workers, when they become available, and scales down workers reliably to free resources again. 


\subsection*{Ability to interact with the WLCG and scale outside of the facility on occasion}

Given the distributed nature of HEP research and the existence of the WLCG, analyzers often face the need to interact with the wider infrastructure. Common tasks may include interacting with other WLCG and other opportunistic resources, pulling or pushing data in and out of the facility. As such, AFs should not be “sealed environments”, but integrated into the wider WLCG infrastructure.  If the workload would exceed traditional analysis activities and/or the data is distributed across the WLCG and the task at hand matches well to grid-style processing the user should be able to use grid middle-ware tools to offload the work to the WLCG and connect to opportunistic resources. The latter may be particularly important for training large machine learning models for which AFs may not have sufficient GPUs. The ability to convert interactive workflows to batch-like jobs (as described above) should also help with this use case.

\subsection*{Ability to efficiently train machine learning models for HEP}

The progress over the last decade in Machine Learning (ML) techniques invariably also affects physics analysis in HEP, where it is used across the board in data-taking, simulation, reconstruction and analysis. ML methods play a dual role: they can be used to find more powerful algorithms, but also enable significant acceleration due to the high degree of parallelization possible in neural networks, which maps well to hardware such as GPUs. For AFs this implies two important requirements: 1) the need for an efficient python-based analysis environment and 2) access to heterogeneous hardware both interactively as well as in batch mode. The first is due to the dominance of python within ML research with tools like PyTorch~\cite{pytorch}, SciKit-Learn~\cite{scikit-learn}, and Tensorflow~\cite{tensorflow}, which is expected to continue in the short to medium term. ML development also requires early on data- and compute-intensive fast iteration during the development of the model, where interactive access to, e.g. Jupyter notebooks is a priority. The second requirement of accelerator availability must be met both interactively as well as non-interactively. As a user, it is expected to be able to start an interactive session with GPU access. For a positive user experience it may be more important to be able to quickly access a single GPU than to be able to keep it for a long time or scale from a single GPU to a multi-GPU environment. Access to batch GPUs is increasingly required for model selection and hyperparameter tunings but also for cutting-edge ML research, because both models and datasets are growing, and a typical training run for many models now exceeds 24 hours. Here the user should be able to use standard workload schedulers to describe the shape of the workload (e.g. GPUs, CPUs, memory). 

\subsection*{Ability to reproducibly instantiate desired software stack}

Analyses are a mixture of both collaboration-wide software frameworks and more custom-tailored packages that suit the particular analysis needs of the team. As a user, it is reasonable to expect the ability to assemble this mixture. As an additional aspect, the ability to archive and possibly distribute this particular software environment is important. This may be of particular interest for both analysis preservation and reuse purposes, but also help towards the ability to reinstantiate the analysis environment at different facilities. CVMFS~\cite{cvmfs} --- the dominant solution in HEP for software distribution --- as well as Linux containers are expected to contribute here. Users may not be experts in these technologies and thus the process of listing requirements for the software environment may need to be abstracted from the underlying technology.

\subsection*{Ability to collaborate in a multi-organisational team on a single resource}

Analysis teams at HEP experiments are multi-organisational: team members may not be at the same institution, region or even country, yet the goal is to jointly work towards a physics result. Analysis is also distributed: responsibility for parts of the analysis workflow is often taken on by a subgroup and the resulting data products of one subgroup must be seamlessly passed onto another one. Such a requirement is easiest to fulfill if access to resources are equitable, where the full team can work with the same resources. The main reason for this is that maintaining copies of data, i.e. downloading datasets, ensuring all files are present, doing necessary post-processing/combining/bookkeeping on the local files before they're ready for analysis is hard work for the users, inefficient and error-prone. Having one copy means this only needs to be done once. In the current model this is ntuples, but this could also refer to skims of reduced formats in the future. 

Given the data volumes of an analysis it may be desirable to work on the same resource for the duration of an analysis project, however, fine-grained control over which resource this would be may not be necessary and may be left up to the VO, i.e. the VO may assign users or physics groups to AFs rather than users flocking to a facility or another. 

\subsection*{Ability to move analyses to new facilities}

While it should be possible for the full analysis team to access the same facility for jointly developing the analysis, users and/or system administrators may value the ability to move an analysis effort between facilities. This implies the requirement that the data products that are local to a single facility can be replicated to distributed storage. Note that this does not imply a requirement for a distributed, fast, automatic synchronization but rather an on-demand method of data movement. 

\subsection*{Ability to efficiently access collaboration data as well as make intermediate data products available to the team}

Analysis starts after a common bulk pre-processing of the data by the collaboration on WLCG resources. It is therefore necessary to be able to interact from within the facility with distributed data management services, such as Rucio~\cite{rucio}, both from interactive and non-interactive environments. Notably, the physical presence of the data on the processing site is not a priority of the user as long as a smooth processing of the data can be ensured on-demand. This opens the possibility of inbound data-access via caching services such as XCache~\cite{xcache}, where the user only references a global identifier and the file is made available on-demand.

During user data processing new data products are being created which need to be made available within the facilities’ distributed storage. Additionally it is important that users are able to share data to other collaborators (possibly working on other facilities). Currently this is often satisfied through an ad-hoc choice of a storage solution that many collaborators have access to (e.g. EOS~\cite{eos}) as a “data exchange”. Ideally, however, it would be possible to register user data seamlessly in a global namespace. It may be sufficient that such data publishing is based on explicit intent and full POSIX semantics may not be needed. Thus an implementation of such a requirement does not imply a global, automatically synchronized filesystem (such as AFS that provides networked file storage for CERN users).
 
\subsection*{Ability to express interdependent distributed computations at small and large scales}

An analysis at LHC scale is often the result of a complex workflow of multiple interdependent steps that need to be executed in a certain order: for example data reduction/event selection, followed by preparation of the inputs required for statistical analysis, followed by inference and visualization steps. Here, it is often the case that due to the distributed nature of analysis teams, steps are largely independently developed by analysis subgroups and later on integrated into an overall data analysis pipeline. The individual steps may have different, and even conflicting, software environment requirements. This implies users do not just need to set up one environment, but ideally seamlessly transition between many. Nonetheless, it is desirable for analyzers to be able to express and schedule the full data analysis pipeline into an overall workflow. 

Workflows in this sense should be contrasted to computational graphs that are, e.g. built during the use of a distributed data processor, such as Dask~\cite{dask}, as each node in the former may be a full analysis step (e.g., running the fit), while the latter typically considers nodes to be small computational units (e.g., compute invariant mass). Such workflows have been used both for analysis reinterpretation work, where the full analysis pipeline is re-executed on new simulation data, as well as for automation of, e.g. the production of calibration functions for analysis. 

\subsection*{Open Questions}

\subsubsection*{On interactivity of analysis sessions}
What is the user expectation of an interactive session? Interactive analysis sessions on large datasets implies the ability to scale out to distributed resources and users will expect instant access to such an analysis session. What are the required disk resources, computing cores (order 100s) and network capabilities to realize this? The answers to these questions are analysis/workflow dependent, vary widely, and will need to be quantified as such. The exact definition of ``interactive analysis" remains an open question for analysis in the HL-LHC era, where datasets will be much larger (~10x) for ATLAS and CMS from Run 4 onwards and will also grow again for LHCb and ALICE from Run 5. This does not automatically mean that interactive needs jump by 10x, but an increase will happen and should be quantified.

\subsubsection*{On the Interfaces to resources provided}
Analysis work is multi-faceted and while a browser-based interface can be attractive for some use-cases such as interactive computing with notebooks, it should not be the only possible interface to the resource. Should users expect to be able to connect via their preferred text-based terminals or Integrated Development Environments (IDEs) such as Microsoft's Visual Studio Code? 



