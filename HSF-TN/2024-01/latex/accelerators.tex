\section{Accelerator  Resources}
\label{accelerators}

The most commonly used accelerators in HEP are GPUs and this is the focus of the discussion here. It is expected that both availability and demand for GPU resources is going to increase, not least in the context of HPC facilities, and for sustainability considerations similar to those made for the introduction of ARM processors on the grid \cite{arm}. It is important that GPU resources be made available both for interactive and non-interactive workloads. Particularly in machine learning, a large fraction of time is spent in R\&D to develop a candidate model interactively, before large-scale non-interactive trainings are launched, e.g. for hyperparameter tuning. 

Aside from pure availability of heterogeneous resources, it is important for AFs to provide software hooks to integrate such hardware. WLCG computing has until recently been based on the assumption of relative uniformity of hardware resources. The exclusive use of x86 architecture guaranteed that user code could run everywhere. GPUs, or accelerators in general, pose a new challenge for AFs and WLCG sites. The variety of GPU models, functionality and software needed to access them, as well as the complex memory management, make GPUs more difficult to insert both in a local context as well as at distributed facilities. The user code is still the subject of R\&D and the requirements, not yet as well defined as for CPUs, vary from GPU sharing, to partial offload of the code to a GPU,  to use of multi-GPUs for large ML models, to requests for specific versions of libraries. How to tune batch systems to support this variety of configurations and possible user requests is still being investigated by sites. For this reason GPUs so far have been used mostly on dedicated resources, such as the HLT farms for very specific workflows \cite{lhcbhltgpu}, or users have requested single (dedicated) interactive GPUs. In the few cases where multiple GPUs have been requested by a single user they tend to be allocated on a single node. Particularly interesting is the case of interactive GPUs which remain idle while the user is coding, especially when used as a notebook resource that is assigned to the session. Time sharing could be considered in these cases. Physical partitioning is also an option in newer GPU models, and permits providing multiple GPU flavors. Some AFs, like Nebraska and FNAL elastic AF, are deploying specialized software, such as NVIDIA's Triton inference servers, for their inference-as-a-service infrastructure in parallel to their batch system access. The Triton server schedules the model requests and can be used from either local HTCondor jobs or from the interactive nodes.

Users also use commercial cloud resources to access GPUs, but this type of access is usually not integrated. As demand for such resources is still very dynamic, it may be interesting to investigate models of AFs that couple with cloud computing resource providers, where such resources can be allocated on-demand. This model has been used by LHC collaborations, such as ATLAS, to investigate the technical compatibility of collaboration software, as well as validation of the physics output, for other types of non-x86 architectures like ARM and this coupling can also be used for accessing various types of accelerators, more commonly GPUs. CMS, LIGO and protoDUNE have also used commercial providers for integrated inference-as-a-service. In these cases the user interface can either be independent or mediated through the experiment grid WFMS (Workflow Management System). 
