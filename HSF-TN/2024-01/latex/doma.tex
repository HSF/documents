\section{DOMA (Data Organisation, Management, Access) }
\label{doma}

Achieving the transparent movement of data between AFs and other storage systems used within HEP is still a matter of development. Fast access to input data is one of the most important aspects of an AF. Considerable effort is going into reducing the input data volume handled by end users so that it can fit comfortably at single sites. However, not all analyses will be able to use such “reduced” formats (the current plan is to have a 70-80\% analysis using them) and whilst the site may have the storage space, it may not have the interactive computing resources for an AF (for example a T1 site). A second aspect that needs to be taken into consideration is where users’ output will be stored and if it needs to be shared. Experiment analysis models are expected to develop such that direct large-scale ntuple productions by analysis groups are very uncommon. The new reduced formats are expected to replace these,  with large productions continuing to be carried out on the grid. Support for access and distribution of data from analysis facilities needs to be guarantueed, i.e. AFs should be taken in consideration in the experiment's data model

\subsection*{Data access: single site, replicated or cached}

Users need to be able to find their input data as they would on any other type of experiment resource and be able to access them in a timely manner. There are a few possibilities to organize the data and these shape the AF infrastructure:

\begin{itemize}
    \item Put everything at CERN. This of course is possible but the interactive and batch system resources should also increase to cope as well as the user support response to the increased requests.
    \item Create other facilities that host  a large storage common to all computing resources and a large amount of interactive/batch resources, similar to those found at CERN. Typically this would be at T1s or large labs like DESY. 
    \item Remote access to all the data from interactive computing resources with the latency hidden by the caches. This already exists at some sites like UCSD and NERSC.
\end{itemize}



The last of these models can maximize the usage of any type of available computing resource and does not stop either of the previous models from being implemented.

Analysis workflows frequently access the same data repeatedly, so caching is beneficial. So far all testing has been done using XRootD~\cite{xrootd} based XCaches~\cite{xcache}, which have two advantages: the first is that the applications (including uproot~\cite{uproot} and RData Frame\cite{rdataframe}) can read only the required portions of files, reducing the volume of transfers; the second is that they are already used at HEP sites so it is just a matter of ironing out the more specific user requirements. Hosting managed copies of the data is usually painful, unless a grid storage system is available and integrated in the experiment’s system. Caches, however, require little maintenance. If the deployment of a cache is simple enough that it can be adapted to be used at Tier3 sites then these resources could continue to be used effectively without hosting managed data. This is particularly important considering the analysts will want local user formats (nanoAOD/PHYSLITE) skimmed (a typical case is  multi-lepton analysis) or augmented (i.e. custom objects that need tracks or particle flow candidates). Augmented files may point to external remote files, so the ability to copy locally the augmentation onto a cache is almost mandatory to reduce latency. There is the possibility to use https for caches, but that is not as well tested and would require further R\&D. Using caches means the HL-LHC data, in whatever format, has to be on remotely accessible storage that can be integrated with the caches, i.e., it cannot be on purely internal storage. There is also a question on the specifications of these caches. They need to be fast, e.g. on SSD and NVMe, and possibly support a large number of users; but they could also be large HDD which are not as fast but store a lot more data. For example the Southern California (SoCal) Cache is a ~2PB cache serving CMS analysis jobs from both UCSD and Caltech and is made out of retired HDDs. The scalability comes from using several data servers grouped under a redirector (similar to a regular XRootD cluster). 

\subsection*{Common namespaces} 

Depending on the preference of the experiment on how to track data access, access to remote data may benefit from integrating the data management tools into the user interface. Usually users query their experiment’s DDM (Distributed Data Management) system  (e.g. Rucio or Dirac~\cite{dirac}) from the command line, but considering the request of interactive resource access via IDE, it is interesting to highlight the integration of Rucio with a Jupyter notebook plug-in. This was developed by Rucio and the ESCAPE~\cite{escape} project and avoids a double step of having to query Rucio in a terminal and then upload the resulting list in the notebook. The plug-in could have configurable caches available. In this case the DDM tools provide the data common namespace. It is also possible to access a common namespace using a combination of XRootD redirectors and caches like CMS does, however here the user still has to know the list of files they want to access, requiring interaction with a file catalog. 

Another example of a common namespace is when site storage underpins all its computing resources. Usually this is a POSIX-like file system. The most prominent example for this is EOS at CERN. EOS offers a POSIX-like file system, with a single namespace, that can be accessed by interactive resources, local batch and cloud resources. Grid jobs and remote non-grid resources can also read and write data via other protocols and the users can always find them easily via the POSIX-like interface. This setup is one of the reasons CERN is so popular for analysis --- everything is easily accessible with a variety of different tools.

\subsection*{POSIX vs Object stores}

Full POSIX compliance cannot be expected and is also very unlikely to be necessary  for complexity and for performance reasons. In this text, POSIX should be implicitly understood as POSIX-like.

Users favor POSIX file systems because they are more familiar and intuitive and the experiments’ applications and services are also built on POSIX. The emergence of object stores, with their scalability and efficiency in serving data, raises the possibility that new workflows can be adapted to use distributed object stores. Scalability of hierarchical POSIX file systems is a recurring worry, and further effort to understand if data access speed and scalability are limited by application logic or storage technologies is needed. One should also clearly distinguish between the use cases: accessing several TB of data from mass storage vs copying a few smaller files in a local directory. One needs to take into consideration the evolution of analysis software, like ROOT. Traditionally it is bound to local IO and relies on POSIX semantics, which may use "advanced" concepts (symlinks, hardlinks, fseek()...) for maximum performance and convenience. Some analysis software may already provide data IO interfaces for simpler protocols (such as Amazon's S3 and XRootD). However even though the tools provide those interfaces, users usually wrap their analysis in their own code that will use a local file system, because that is what they are familiar with. For example, a user may create a script to chunk a data file into smaller files relying on local Unix tools rather than learning how to do that in S3. Users may also want to explore a new dataset or download some data to their local computers in a format with which they are familiar, which is static files with POSIX semantics for access/interaction. This happens on a small scale: users are not interactively exploring or exporting terabytes to petabytes of data. Therefore, the only requirement is that an object store can export (usually small amounts of) selected data to a static file. Similarly, user output (histograms, etc.) from analysis workflows is orders of magnitude smaller and may be input to subsequent tasks that are separate from the AF and any object store semantics. Therefore, the ability to output static files is useful, but separate from storing, accessing, and caching TB-PB of data. "Skimming", keeping only selected subset of events for further analysis, is an intermediate case, and one probably wants the option to output into an object store format for this case. Taken together, all of this implies a general need for both static files and object stores to be accepted as both input and output formats, with the most efficient formats preferred for the largest-scale processing tasks. 

There have been a few developments in applications looking to support object stores like the ROOT I/O system RNtuple~\cite{rntuple} or XRootD framework. However a wider shift to object stores would require a dedicated working group to assess the pros and cons of integrating them into experiment software. In the future it could be possible to access large scale object stores directly from the analysis software while continuing to support POSIX-like features for smaller more interactive tasks. 

Whatever architecture an AF may eventually have (distributed, replicated at T1s, integrated with national resources) a storage system that is accessible from everywhere and can be easily replicated should be the aim. Different sites may have their own solutions, and often a different type of storage for different resources with different protocols, this poses a challenge for users to migrate to other facilities or share their data. Currently this role is filled by the grid storage which is not designed for user experience, particularly without going through the experiments’ DDM.

\subsection*{Integration of namespaces and access methods}

As expressed in the user requirements, users want the ability to move from one resource to another. This implies the ability to share the data as well as the code with colleagues. For this   a possible solution may be for facilities to offer both local storage, that computer nodes can read and write to and from efficiently, as well as storage whose contents will be registered in the wider DDM infrastructure. While DDM tools such as Rucio support user uploads, their current main use is the upload of output data from WLCG jobs, so it would be necessary to extend this to analysis facility output datasets. 

For many analysis activities users currently use file systems available at well-known sites, such as EOS, to share data. Within the HSF AF Forum the idea of a global and shareable working space, has been discussed. However, it would be necessary for such infrastructure to be extendable to GB-TB scale datasets that may be produced during analysis. There is no obvious scalable and performant solution here. We cannot resort to distributed file systems like AFS for performance reasons, sharing services like CERNbox~\cite{cernbox} are not suited for large data sharing  and “common spaces” on grid storages need easier interfaces for the users.

There are all sorts of details to be ironed out to support storage systems with different namespaces and access methods. Whether talking about POSIX vs object stores, grid storage vs local storage, different access methods at the same site or integrating a WEB based service, like CERNbox, the problems are similar. It will require work to map each solution to the access of another, for example how CERN integrates SSO and Kerberos for Binder~\cite{binder}. Some solutions may work only for fuse mountable protocols (EOS/XRootD) and not for filesystems with kernel drivers. There is also the option adopted by LHCb to encourage users to use their Analysis Production Data package (apd)~\cite{apd} tools to find data also locally which ATLAS and CMS could do using Rucio.   Alternatively, moving away from POSIX has been proposed, but this clashes with the fact that POSIX is at the base of any Linux server and to avoid it we should forbid users from interacting with file systems, which is not feasible. Going forward if more cloud technologies are introduced it will be easier to integrate object stores from the AAI point of view but also from the infrastructure point of view. For example, whereas a broken POSIX/fuse mount is very intrusive to workloads, object stores are more loosely coupled and make it easier to recover from issues with the backends. Regarding integration with containers, whereas POSIX and fuse often require additional components and even additional privileges, object stores, being native cloud storage, integrate very well. 

\subsection*{CERNBox data and document sharing}

File sharing services like CERNBox, heavily used for data and document sharing, can benefit from the fact that the output data of jobs run at the CERN facility can be written on a dedicated EOS space, making them automatically available to colleagues. CERNBox has other functionalities, such as integration with Zenodo that makes it really interesting as a document sharing service. Another useful workflow that CERNBox enables is to combine online work at the AF with offline work in a personal computer, and have the files synchronized via the CERNBox sync client installed on the machine. The CERNbox team is currently extending support beyond EOS to CephFS which will make the service deployable at more sites. To make it really as useful as the CERN instance though, the underlying storage should allow users to write with standard protocols data that can be shared via CERNbox. Of course other labs may have a file sharing service but again we face the access problem: an external person cannot use it without an account or a token and questions about interoperability need to be considered; furthermore such share-and-sync services usually can provide only modest storage with limited integration, making it mostly applicable to small, final outputs.

\subsection*{Data transformation services}


An important step of an analysis is to transform data from one format to another or to create a custom selection of the data. Examples include: physics groups produce ntuples and then users can further refine the event selections; users developing new ML algorithms and parallelized workflows may want to transform ROOT files to other formats developed outside of HEP for ease of use with other tools; users may need to augment existing formats with custom information. Usually these steps are carried out in isolation, but a logical goal is to simplify and reduce the number of steps required, particularly if they have to be carried out on different resources, i.e. grid first and then local resources and require to register and manage transformed copies and take more storage.  Transformation services, such as ServiceX~\cite{servicex}, have been introduced to give users the ability to transform data on the fly. Such services need to be fully integrated with performant storage and caches to serve the data in a timely manner and could be prime consumers of the object stores mentioned in the previous section. This introduces a dependency on new intermediate services that need to serve multiple users and cache intermediate data for further re-use. The efficiency of these intermediate services are being tested to understand if they are a viable solution. 

\subsection*{Open Questions}

\subsubsection*{On data “locality” at facilities}

Should analysts expect the data, particularly reduced formats, to be local to any facility they wish to use (thus providing low latency access)? Often analysts produce derived datasets (with extra cuts, derived variables) and work with them, does the same apply? 

\subsubsection*{On POSIX file access?}
Is POSIX required? Maybe just interacting with an object store via e..g XRootD / https / analysis software is sufficient? How much work is required to fully support object stores? Users like filesystem-like semantics, but what part of POSIX is really needed and can we decouple mass storage access from more interactive, smaller scale activities?

\subsubsection*{On user interaction with Distributed Data Management systems}
Can all Distributed Data Management queries be hidden from the user and is this desirable? Should users expect that this is managed for them? 

\subsubsection*{On provenance of intermediate data products}
Which intermediate files need to be promoted from local to global storage so that users can run at different sites and how is this declared by the user? 

\subsubsection*{On common file sharing services}
Do we need a common file sharing service to help users share their files? In which case can any existing services fulfill this purpose? 


